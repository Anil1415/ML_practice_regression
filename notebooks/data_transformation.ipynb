{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import os\n",
    "from src.utils import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path = os.path.join('artifacts', 'preprocessor.pkl')\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self) -> None:\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "\n",
    "    def get_data_transformation_objects(self):\n",
    "        try:\n",
    "            logging.info('Data Transformation initiated')\n",
    "           # Define which columns should be ordinal-encoded and which should be scaled\n",
    "            categorical_columns = ['cut', 'color','clarity']\n",
    "            numerical_columns = ['carat', 'depth','table', 'x', 'y', 'z']\n",
    "            # define the custom ranking for each ordinal variable\n",
    "\n",
    "            cut_categories = ['Fair','Good','Very Good', 'Premium', 'Ideal']\n",
    "            color_categories = ['D','E', 'F', 'G', 'H', 'I', 'J']\n",
    "            clarity_categories = ['I1','SI2', 'SI1',  'VS2' ,'VS1','VVS2','VVS1','IF']\n",
    "\n",
    "            logging.info('Pipeline Initiated')\n",
    "\n",
    "            # creating numerical pipeline\n",
    "\n",
    "            numerical_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Sequentially apply a list of transforms and a final estimator\n",
    "            categorical_pipeline =Pipeline(\n",
    "                steps=[\n",
    "                ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinalencoder', OrdinalEncoder(categories= [cut_categories,color_categories,clarity_categories])), # sequence array_like \n",
    "                ('scaler', StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            preprocessor = ColumnTransformer([\n",
    "            ('num_pipeline',numerical_pipeline,[i for i in numerical_columns] ), #should pass array for columns Passed in list comprehension\n",
    "            ('cat_pipeline', categorical_pipeline, [i for i in categorical_columns])\n",
    "            ])\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "            logging.info('pipeline completed')\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error in Data Transformation\")\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df =pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            logging.info('Read train and test data completed')\n",
    "            logging.info(f'Train Dateframe Head: \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test Dataframe Head : \\n{test_df.head().to_string()}')\n",
    "\n",
    "            logging.info('Obtaining preprocessing object')\n",
    "\n",
    "            preprocessing_obj = self.get_data_transformation_objects()\n",
    "\n",
    "            target_column_name = 'price'\n",
    "            drop_columns = [target_column_name, 'id']\n",
    "\n",
    "            input_feature_train_df = train_df.drop(columns = drop_columns, axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=drop_columns, axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            ## Transforming using preprocessor obj\n",
    "\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            logging.info('Applying preprocessing object on training and test datasets')\n",
    "\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            save_object(\n",
    "                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj = preprocessing_obj\n",
    "            )\n",
    "            logging.info('preprocessor pickle file saved')\n",
    "\n",
    "            return(train_arr,\n",
    "                test_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info('Exception occured in the initiate_datatransformation')\n",
    "            raise CustomException(e, sys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "303338e15be7ff4e8279020f431c66508b7e8d96b0ee1a1e002248bd9697861f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
